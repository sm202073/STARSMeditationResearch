{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Paths for all the different corpora\n",
    "folderpaths = ['../Corpora/AnxietyMeditations/', '../Corpora/SleepMeditations/',\n",
    "               '../Corpora/LearningAndGrowthMeditations/', '../Corpora/MorningMeditations/']\n",
    "\n",
    "#array to store the text of each file \n",
    "corpus = []\n",
    "text2 = \"\"\n",
    "\n",
    "for folder in folderpaths:\n",
    "    if folder == '../Corpora/AnxietyMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "                text2 += text\n",
    "    elif folder == '../Corpora/SleepMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "                text2 += text\n",
    "    elif folder == '../Corpora/LearningAndGrowthMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "                text2 += text\n",
    "    elif folder == '../Corpora/MorningMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "                text2 += text\n",
    "\n",
    "newStopWords = ['[', ']', '`', '#', '$',\n",
    "                '(', ')', \"Music\", ':', \"....\", \"--\", \",\", \"''\", '`', '\"']\n",
    "stopwords = set(stopwords.words('english') +\n",
    "                newStopWords + list(string.punctuation))\n",
    "\n",
    "# Tokenizing by word and filtering stopwords\n",
    "all_tokens = [w for w in word_tokenize(text2.lower()) if not w in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Preprocess the text data\n",
    "tokenized_corpus = []\n",
    "for text in corpus:\n",
    "    # Tokenize and lowercase\n",
    "    tokens = [w for w in word_tokenize(text.lower()) if not w in stopwords]\n",
    "    tokenized_corpus.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04611077  0.05143551  0.00832617  0.10568035  0.0678573  -0.05113725\n",
      "  0.08877496  0.05135308 -0.0924836   0.00625963 -0.00269381 -0.10847999\n",
      "  0.01868773 -0.0461509   0.01276269  0.04809852 -0.01082935 -0.08588783\n",
      " -0.01512117 -0.22306041 -0.00630978  0.09596051  0.10741212 -0.06557321\n",
      " -0.01084703  0.06969886 -0.04669424 -0.00948677 -0.12056852  0.00332942\n",
      "  0.09238226  0.03065117  0.0625176  -0.01604205 -0.10206982  0.09264807\n",
      " -0.00654783  0.00070246 -0.05666429 -0.12213346  0.09708432 -0.02240049\n",
      " -0.02771159 -0.06025636  0.06934972 -0.04438044 -0.06740531  0.07275707\n",
      "  0.0979266   0.03026421 -0.01174417 -0.01931076  0.03403663  0.02161161\n",
      " -0.05970366  0.04387289  0.00672422 -0.07817086 -0.01326801 -0.03930199\n",
      "  0.00552479  0.05517378  0.09445991  0.01742239 -0.04255137  0.0171051\n",
      " -0.11576808  0.02523129 -0.09439326 -0.01464783 -0.00791736  0.09378819\n",
      "  0.02240686 -0.02210553  0.02032431 -0.02899616  0.02707856 -0.09984975\n",
      " -0.03723887 -0.0573241  -0.00680383 -0.06865994 -0.0375613   0.0239954\n",
      "  0.02726975 -0.04742156 -0.03341101 -0.00549951  0.04620064  0.01876239\n",
      "  0.02586895  0.12089632 -0.04524095  0.0392011   0.14971869  0.0350569\n",
      " -0.03226656 -0.05825174  0.06922968 -0.01577401]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "# Preprocess the text data\n",
    "tokenized_corpus = []\n",
    "for text in corpus:\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    tokenized_corpus.append(tokens)\n",
    "\n",
    "context_windows = []\n",
    "window_size = 4  # Size of the context window\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    sentence_length = len(sentence)\n",
    "    for i in range(sentence_length):\n",
    "        # Extract the target word\n",
    "        target_word = sentence[i]\n",
    "\n",
    "        # Extract the context words within the window size\n",
    "        context_words = []\n",
    "        for j in range(max(0, i - window_size), min(sentence_length, i + window_size + 1)):\n",
    "            if j != i:\n",
    "                context_words.append(sentence[j])\n",
    "\n",
    "        # Append the target word and its context words as a tuple\n",
    "        # Convert context_words to tuple\n",
    "        context_windows.append((target_word, tuple(context_words)))\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(context_windows, window=5,\n",
    "                 min_count=10, sg=0)  # CBOW approach\n",
    "\n",
    "# Accessing trained word vectors\n",
    "word_vector = model.wv['breathe']  # Get the word vector for a specific word\n",
    "print(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define the directory containing the text files\n",
    "directory = \"./txt Files/\"\n",
    "\n",
    "all_vectors = { 'sight': [], 'smell': [], 'sound': [], 'taste': []\n",
    ", 'touch': []}\n",
    "\n",
    "# Iterate over the files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, \"r\") as f:\n",
    "            text = f.read()\n",
    "            words = nltk.word_tokenize(text)\n",
    "            for word in words:\n",
    "                try:\n",
    "                    word_vector = model.wv[word.lower()]\n",
    "                    all_vectors[filename[:-4]].append(word_vector)\n",
    "                    # print(f\"Word: {word}\\nVector: {word_vector}\\n\")\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                    # print(f\"Word '{word}' not in vocabulary.\\n\")\n",
    "\n",
    "# print(len(all_vectors['sight']))\n",
    "# print(len(all_vectors['sight']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(10, 10)\n",
      "(10, 10)\n",
      "(10, 10)\n",
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "for key in all_vectors.keys(): \n",
    "    # Calculate the pairwise cosine similarity distance between each row in the matrix\n",
    "    sight_len = len(all_vectors[key])\n",
    "    distances = np.zeros((sight_len, sight_len))\n",
    "    for i in range(sight_len):\n",
    "        for j in range(i+1, sight_len):\n",
    "            distances[i][j] = cosine(all_vectors[key][i], all_vectors[key][j])\n",
    "            distances[j][i] = distances[i][j]\n",
    "\n",
    "    # Print the resulting distance matrix\n",
    "    distances = distances[:10, :10]\n",
    "    print(distances.shape)\n",
    "\n",
    "# Normalize the matrix to the range 0-1\n",
    "# normalized_distance = (distances - np.min(distances)) / (np.max(distances) - np.min(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
