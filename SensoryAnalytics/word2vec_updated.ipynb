{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Paths for all the different corpora\n",
    "folderpaths = ['../Corpora/AnxietyMeditations/', '../Corpora/SleepMeditations/',\n",
    "               '../Corpora/LearningAndGrowthMeditations/', '../Corpora/MorningMeditations/']\n",
    "\n",
    "#array to store the text of each file \n",
    "corpus = []\n",
    "text2 = \"\"\n",
    "\n",
    "for folder in folderpaths:\n",
    "    if folder == '../Corpora/AnxietyMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "                text2 += text\n",
    "    elif folder == '../Corpora/SleepMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "                text2 += text\n",
    "    elif folder == '../Corpora/LearningAndGrowthMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "                text2 += text\n",
    "    elif folder == '../Corpora/MorningMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "                text2 += text\n",
    "\n",
    "newStopWords = ['[', ']', '`', '#', '$',\n",
    "                '(', ')', \"Music\", ':', \"....\", \"--\", \",\", \"''\", '`', '\"']\n",
    "stopwords = set(stopwords.words('english') +\n",
    "                newStopWords + list(string.punctuation))\n",
    "\n",
    "# Tokenizing by word and filtering stopwords\n",
    "all_tokens = [w for w in word_tokenize(text2.lower()) if not w in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Preprocess the text data\n",
    "tokenized_corpus = []\n",
    "for text in corpus:\n",
    "    # Tokenize and lowercase\n",
    "    tokens = [w for w in word_tokenize(text.lower()) if not w in stopwords]\n",
    "    tokenized_corpus.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02571531  0.06281108  0.02818606  0.11388033  0.06083102 -0.04686197\n",
      "  0.09241629  0.03582029 -0.08555122 -0.00934544 -0.00797304 -0.11676246\n",
      "  0.01230918 -0.06387286  0.00378325  0.02980082 -0.01165971 -0.08301544\n",
      " -0.02056445 -0.21911184 -0.00169653  0.08718224  0.08829284 -0.06478397\n",
      " -0.02349795  0.08676254 -0.04553212 -0.02165003 -0.12373185  0.01578454\n",
      "  0.079062    0.03106468  0.05743916 -0.01540777 -0.10006491  0.08969834\n",
      "  0.00525573 -0.00038158 -0.05550962 -0.12587121  0.09258448 -0.03257527\n",
      " -0.01509936 -0.05683074  0.09033899 -0.03751928 -0.07906368  0.06086323\n",
      "  0.0907647   0.01035591 -0.01625794 -0.0197103   0.01278726  0.00644125\n",
      " -0.0809281   0.06339431  0.00782283 -0.07654206 -0.02362784 -0.02029045\n",
      "  0.020501    0.04719731  0.07319804  0.02503927 -0.02815674  0.03559386\n",
      " -0.12134167  0.02501273 -0.09099378 -0.03016839 -0.01211414  0.08106922\n",
      "  0.01969916 -0.02452764  0.03276649 -0.03212637  0.03407028 -0.09734505\n",
      " -0.04621753 -0.06529494 -0.00386717 -0.05495413 -0.03855034  0.02491293\n",
      "  0.0065131  -0.05544085 -0.03921475 -0.00502595  0.04290172 -0.00416456\n",
      "  0.02707571  0.10531167 -0.06466287  0.02338334  0.12806228  0.02559102\n",
      " -0.03653605 -0.08402353  0.06535807 -0.01044313]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "# Preprocess the text data\n",
    "tokenized_corpus = []\n",
    "for text in corpus:\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    tokenized_corpus.append(tokens)\n",
    "\n",
    "context_windows = []\n",
    "window_size = 4  # Size of the context window\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    sentence_length = len(sentence)\n",
    "    for i in range(sentence_length):\n",
    "        # Extract the target word\n",
    "        target_word = sentence[i]\n",
    "\n",
    "        # Extract the context words within the window size\n",
    "        context_words = []\n",
    "        for j in range(max(0, i - window_size), min(sentence_length, i + window_size + 1)):\n",
    "            if j != i:\n",
    "                context_words.append(sentence[j])\n",
    "\n",
    "        # Append the target word and its context words as a tuple\n",
    "        # Convert context_words to tuple\n",
    "        context_windows.append((target_word, tuple(context_words)))\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(context_windows, window=5,\n",
    "                 min_count=10, sg=0)  # CBOW approach\n",
    "\n",
    "# Accessing trained word vectors\n",
    "word_vector = model.wv['breathe']  # Get the word vector for a specific word\n",
    "print(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define the directory containing the text files\n",
    "directory = \"./txt Files/\"\n",
    "\n",
    "all_vectors = { 'sight': [], 'smell': [], 'sound': [], 'taste': []\n",
    ", 'touch': []}\n",
    "\n",
    "# Iterate over the files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, \"r\") as f:\n",
    "            text = f.read()\n",
    "            words = nltk.word_tokenize(text)\n",
    "            for word in words:\n",
    "                try:\n",
    "                    word_vector = model.wv[word.lower()]\n",
    "                    all_vectors[filename[:-4]].append(word_vector)\n",
    "                    # print(f\"Word: {word}\\nVector: {word_vector}\\n\")\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                    # print(f\"Word '{word}' not in vocabulary.\\n\")\n",
    "\n",
    "# print(len(all_vectors['sight']))\n",
    "# print(len(all_vectors['sight']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 15)\n",
      "(15, 15)\n",
      "(15, 15)\n",
      "(15, 15)\n",
      "(15, 15)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "for key in all_vectors.keys(): \n",
    "    # Calculate the pairwise cosine similarity distance between each row in the matrix\n",
    "    sight_len = len(all_vectors[key])\n",
    "    distances = np.zeros((sight_len, sight_len))\n",
    "    for i in range(sight_len):\n",
    "        for j in range(i+1, sight_len):\n",
    "            distances[i][j] = cosine(all_vectors[key][i], all_vectors[key][j])\n",
    "            distances[j][i] = distances[i][j]\n",
    "\n",
    "    # Print the resulting distance matrix\n",
    "    distances = distances[:15, :15]\n",
    "    print(distances.shape)\n",
    "\n",
    "# Normalize the matrix to the range 0-1\n",
    "# normalized_distance = (distances - np.min(distances)) / (np.max(distances) - np.min(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
