{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Paths for all the different corpora\n",
    "folderpaths = ['../AnxietyMeditations/', '../SleepMeditations/',\n",
    "               '../LearningAndGrowthMeditations/', '../MorningMeditations/']\n",
    "\n",
    "#array to store the text of each file \n",
    "corpus = []\n",
    "\n",
    "for folder in folderpaths:\n",
    "    if folder == '../AnxietyMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "    elif folder == '../SleepMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "    elif folder == '../LearningAndGrowthMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "    elif folder == '../MorningMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "\n",
    "newStopWords = ['[', ']', '`', '#', '$',\n",
    "                '(', ')', \"Music\", ':', \"....\", \"--\", \",\", \"''\", '`', '\"']\n",
    "stopwords = set(stopwords.words('english') +\n",
    "                newStopWords + list(string.punctuation))\n",
    "\n",
    "# Tokenizing by word and filtering stopwords\n",
    "all_tokens = [w for w in word_tokenize(text.lower()) if not w in stopwords]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Preprocess the text data\n",
    "tokenized_corpus = []\n",
    "for text in corpus:\n",
    "    # Tokenize and lowercase\n",
    "    tokens = [w for w in word_tokenize(text.lower()) if not w in stopwords]\n",
    "    tokenized_corpus.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04357344  0.06469832  0.00708946  0.10132951  0.04842198 -0.04448317\n",
      "  0.09263577  0.04414168 -0.10461225  0.00455453 -0.01399897 -0.09782488\n",
      "  0.01634117 -0.04093378  0.01624235  0.04146756  0.00205488 -0.07824838\n",
      " -0.02845545 -0.22111169  0.00766507  0.08452726  0.09406016 -0.06352351\n",
      " -0.02355131  0.09575079 -0.05538119 -0.01975208 -0.1269004   0.01042841\n",
      "  0.08245099  0.03363612  0.04226923 -0.00592274 -0.11145383  0.09136933\n",
      " -0.01363419  0.00932562 -0.04875571 -0.12297434  0.09713392 -0.02909073\n",
      " -0.01400315 -0.05426046  0.07613517 -0.03536762 -0.06918352  0.05822345\n",
      "  0.09930514  0.01208888 -0.01047047 -0.02739201  0.01334653  0.01012277\n",
      " -0.07499318  0.05187471  0.00415072 -0.07271872 -0.04003773 -0.03871902\n",
      "  0.02842201  0.04550796  0.06908636  0.02626394 -0.03112886  0.02367999\n",
      " -0.10433532  0.02260037 -0.0981443  -0.01794084  0.00393988  0.08520032\n",
      "  0.0048525  -0.03539305  0.01660635 -0.01464625  0.0289981  -0.08286708\n",
      " -0.04124979 -0.04354507  0.0039212  -0.0758892  -0.03147601  0.01100126\n",
      "  0.0100152  -0.05670233 -0.02851456 -0.00969525  0.03354023  0.01387125\n",
      "  0.03159516  0.10072269 -0.04951141  0.02941632  0.13221875  0.02744729\n",
      " -0.02191003 -0.06918357  0.05257647  0.00105844]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "# Preprocess the text data\n",
    "tokenized_corpus = []\n",
    "for text in corpus:\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    tokenized_corpus.append(tokens)\n",
    "\n",
    "context_windows = []\n",
    "window_size = 4  # Size of the context window\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    sentence_length = len(sentence)\n",
    "    for i in range(sentence_length):\n",
    "        # Extract the target word\n",
    "        target_word = sentence[i]\n",
    "\n",
    "        # Extract the context words within the window size\n",
    "        context_words = []\n",
    "        for j in range(max(0, i - window_size), min(sentence_length, i + window_size + 1)):\n",
    "            if j != i:\n",
    "                context_words.append(sentence[j])\n",
    "\n",
    "        # Append the target word and its context words as a tuple\n",
    "        # Convert context_words to tuple\n",
    "        context_windows.append((target_word, tuple(context_words)))\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(context_windows, window=5,\n",
    "                 min_count=10, sg=0)  # CBOW approach\n",
    "\n",
    "# Accessing trained word vectors\n",
    "word_vector = model.wv['breathe']  # Get the word vector for a specific word\n",
    "print(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define the directory containing the text files\n",
    "directory = \"./txt Files/\"\n",
    "\n",
    "all_vectors = { 'sight': [], 'smell': [], 'sound': [], 'taste': []\n",
    ", 'touch': []}\n",
    "\n",
    "# Iterate over the files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, \"r\") as f:\n",
    "            text = f.read()\n",
    "            words = nltk.word_tokenize(text)\n",
    "            for word in words:\n",
    "                try:\n",
    "                    word_vector = model.wv[word.lower()]\n",
    "                    all_vectors[filename[:-4]].append(word_vector)\n",
    "                    # print(f\"Word: {word}\\nVector: {word_vector}\\n\")\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                    # print(f\"Word '{word}' not in vocabulary.\\n\")\n",
    "\n",
    "# print(len(all_vectors['sight']))\n",
    "# print(len(all_vectors['sight']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 15)\n",
      "(15, 15)\n",
      "(15, 15)\n",
      "(15, 15)\n",
      "(15, 15)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "for key in all_vectors.keys(): \n",
    "    # Calculate the pairwise cosine similarity distance between each row in the matrix\n",
    "    sight_len = len(all_vectors[key])\n",
    "    distances = np.zeros((sight_len, sight_len))\n",
    "    for i in range(sight_len):\n",
    "        for j in range(i+1, sight_len):\n",
    "            distances[i][j] = cosine(all_vectors[key][i], all_vectors[key][j])\n",
    "            distances[j][i] = distances[i][j]\n",
    "\n",
    "    # Print the resulting distance matrix\n",
    "    distances = distances[:15, :15]\n",
    "    print(distances.shape)\n",
    "\n",
    "# Normalize the matrix to the range 0-1\n",
    "# normalized_distance = (distances - np.min(distances)) / (np.max(distances) - np.min(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
