{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Paths for all the different corpora\n",
    "folderpaths = ['../AnxietyMeditations/', '../SleepMeditations/',\n",
    "               '../LearningAndGrowthMeditations/', '../MorningMeditations/']\n",
    "\n",
    "#array to store the text of each file \n",
    "corpus = []\n",
    "\n",
    "for folder in folderpaths:\n",
    "    if folder == '../AnxietyMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "    elif folder == '../SleepMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "    elif folder == '../LearningAndGrowthMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "    elif folder == '../MorningMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "\n",
    "newStopWords = ['[', ']', '`', '#', '$',\n",
    "                '(', ')', \"Music\", ':', \"....\", \"--\", \",\", \"''\", '`', '\"']\n",
    "stopwords = set(stopwords.words('english') +\n",
    "                newStopWords + list(string.punctuation))\n",
    "\n",
    "# Tokenizing by word and filtering stopwords\n",
    "all_tokens = [w for w in word_tokenize(text.lower()) if not w in stopwords]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Preprocess the text data\n",
    "tokenized_corpus = []\n",
    "for text in corpus:\n",
    "    # Tokenize and lowercase\n",
    "    tokens = [w for w in word_tokenize(text.lower()) if not w in stopwords]\n",
    "    tokenized_corpus.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05216778  0.04700494  0.00467626  0.11602146  0.06161752 -0.05302056\n",
      "  0.09786946  0.06228751 -0.11274248 -0.00464894 -0.00156548 -0.11005984\n",
      "  0.01198301 -0.03416103 -0.01878685  0.05118901  0.00337715 -0.10038635\n",
      " -0.02047516 -0.23186155 -0.02321271  0.10729339  0.11130142 -0.06880401\n",
      " -0.01948138  0.08279314 -0.05761955 -0.00093203 -0.13145903  0.00318746\n",
      "  0.08309339  0.04528908  0.06778198  0.01051673 -0.11951401  0.11049\n",
      " -0.00174817  0.00416544 -0.06486303 -0.13929689  0.10906863 -0.01826561\n",
      " -0.02472208 -0.07160486  0.08340048 -0.02346409 -0.09259065  0.07561697\n",
      "  0.11196625  0.03228802 -0.01867998 -0.01492263  0.0291295   0.0139758\n",
      " -0.06425641  0.06143889 -0.01537704 -0.10174065 -0.01343953 -0.04914239\n",
      "  0.03370908  0.05734982  0.09711124  0.0171025  -0.05078335  0.02788507\n",
      " -0.12220423  0.019498   -0.11136904 -0.02480489 -0.01029641  0.09406907\n",
      "  0.00445146 -0.03897272  0.03305973 -0.03689892  0.03455167 -0.10253716\n",
      " -0.04637808 -0.06523985 -0.00366844 -0.06702595 -0.05192767  0.02547424\n",
      "  0.02430996 -0.06439459 -0.03382395 -0.0144028   0.04557821  0.02055286\n",
      "  0.02072689  0.11274467 -0.06414507  0.02702716  0.14187478  0.02151297\n",
      " -0.02736785 -0.08022604  0.08517344  0.00913176]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "# Preprocess the text data\n",
    "tokenized_corpus = []\n",
    "for text in corpus:\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    tokenized_corpus.append(tokens)\n",
    "\n",
    "context_windows = []\n",
    "window_size = 4  # Size of the context window\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    sentence_length = len(sentence)\n",
    "    for i in range(sentence_length):\n",
    "        # Extract the target word\n",
    "        target_word = sentence[i]\n",
    "\n",
    "        # Extract the context words within the window size\n",
    "        context_words = []\n",
    "        for j in range(max(0, i - window_size), min(sentence_length, i + window_size + 1)):\n",
    "            if j != i:\n",
    "                context_words.append(sentence[j])\n",
    "\n",
    "        # Append the target word and its context words as a tuple\n",
    "        # Convert context_words to tuple\n",
    "        context_windows.append((target_word, tuple(context_words)))\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(context_windows, window=5,\n",
    "                 min_count=10, sg=0)  # CBOW approach\n",
    "\n",
    "# Accessing trained word vectors\n",
    "word_vector = model.wv['breathe']  # Get the word vector for a specific word\n",
    "print(word_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "hypernym_file = \"hypernyms.txt\"\n",
    "\n",
    "all_vectors = {}\n",
    "\n",
    "with open(hypernym_file, \"r\") as file:\n",
    "    text = file.read()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    for word in words:\n",
    "        try:\n",
    "            word_vector = model.wv[word.lower()]\n",
    "            all_vectors[word] = word_vector\n",
    "            # print(f\"Word: {word}\\nVector: {word_vector}\\n\")\n",
    "        except KeyError:\n",
    "            continue\n",
    "            # print(f\"Word '{word}' not in vocabulary.\\n\")\n",
    "\n",
    "print(len(all_vectors.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(138, 100)\n",
      "[[ 0.99999994  0.03134152 -0.06566384 ...  0.01753318  0.02094397\n",
      "   0.0768731 ]\n",
      " [ 0.03134153  0.99999994 -0.07252886 ...  0.08610769 -0.04806319\n",
      "  -0.02930969]\n",
      " [-0.06566384 -0.07252886  1.0000001  ...  0.01336481 -0.06905692\n",
      "  -0.11288552]\n",
      " ...\n",
      " [ 0.01753318  0.08610769  0.01336481 ...  0.99999976 -0.02279484\n",
      "  -0.04243451]\n",
      " [ 0.02094397 -0.04806319 -0.06905693 ... -0.02279484  0.99999994\n",
      "  -0.11589417]\n",
      " [ 0.0768731  -0.02930969 -0.11288552 ... -0.04243451 -0.11589417\n",
      "   1.0000001 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "vectors_array = np.array(list(all_vectors.values()))\n",
    "print(vectors_array.shape)\n",
    "\n",
    "# Compute the cosine similarity between all pairs of vectors\n",
    "cosine_sim = np.dot(vectors_array, vectors_array.T)\n",
    "cosine_sim /= np.linalg.norm(vectors_array, axis=1, keepdims=True)\n",
    "cosine_sim /= np.linalg.norm(vectors_array, axis=1, keepdims=True).T\n",
    "\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Hear' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/eeshabarua/Desktop/SP23/CS199/STARSMeditationResearch/SensoryAnalytics/word2vec.ipynb Cell 4\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eeshabarua/Desktop/SP23/CS199/STARSMeditationResearch/SensoryAnalytics/word2vec.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m             \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eeshabarua/Desktop/SP23/CS199/STARSMeditationResearch/SensoryAnalytics/word2vec.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                 curr_dict \u001b[39m=\u001b[39m sense_dicts[sense_index]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eeshabarua/Desktop/SP23/CS199/STARSMeditationResearch/SensoryAnalytics/word2vec.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                 curr_dict[word] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mwv[word]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eeshabarua/Desktop/SP23/CS199/STARSMeditationResearch/SensoryAnalytics/word2vec.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (key, value) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sight_vectors\u001b[39m.\u001b[39mitems()):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eeshabarua/Desktop/SP23/CS199/STARSMeditationResearch/SensoryAnalytics/word2vec.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mprint\u001b[39m(key, value)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_vector(key_or_keys)\n\u001b[1;32m    405\u001b[0m \u001b[39mreturn\u001b[39;00m vstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_vector(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_vector\u001b[39m(\u001b[39mself\u001b[39m, key, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m     \u001b[39m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_index(key)\n\u001b[1;32m    447\u001b[0m     \u001b[39mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'Hear' not present\""
     ]
    }
   ],
   "source": [
    "# # make a dictionary mapping the word to vector for each sense \n",
    "# # to be able to calculate pairwise distance\n",
    "\n",
    "\n",
    "# # read through all the seed word files for each sense\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "directory_path = \"./txt Files/\"\n",
    "\n",
    "\n",
    "sight_vectors = {}\n",
    "smell_vectors = {}\n",
    "sound_vectors = {}\n",
    "taste_vectors = {}\n",
    "touch_vectors = {}\n",
    "sense_dicts = [sight_vectors, smell_vectors,\n",
    "               sound_vectors, taste_vectors, touch_vectors]\n",
    "\n",
    "sense_index = 0\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            text = file.read()\n",
    "            words = nltk.word_tokenize(text)\n",
    "\n",
    "            for word in words:\n",
    "                curr_dict = sense_dicts[sense_index]\n",
    "                curr_dict[word] = model.wv[word]\n",
    "\n",
    "\n",
    "# for i, (key, value) in enumerate(sight_vectors.items()):\n",
    "#     print(key, value)\n",
    "#     if i == 9:\n",
    "#         break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
