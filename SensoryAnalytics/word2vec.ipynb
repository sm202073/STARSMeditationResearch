{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Paths for all the different corpora\n",
    "folderpaths = ['../AnxietyMeditations/', '../SleepMeditations/',\n",
    "               '../LearningAndGrowthMeditations/', '../MorningMeditations/']\n",
    "\n",
    "#array to store the text of each file \n",
    "corpus = []\n",
    "\n",
    "for folder in folderpaths:\n",
    "    if folder == '../AnxietyMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "    elif folder == '../SleepMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "    elif folder == '../LearningAndGrowthMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "    elif folder == '../MorningMeditations/':\n",
    "        for doc in glob.glob(os.path.join(folder, '*.txt')):\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "                corpus.append(text)\n",
    "\n",
    "newStopWords = ['[', ']', '`', '#', '$',\n",
    "                '(', ')', \"Music\", ':', \"....\", \"--\", \",\", \"''\", '`', '\"']\n",
    "stopwords = set(stopwords.words('english') +\n",
    "                newStopWords + list(string.punctuation))\n",
    "\n",
    "# Tokenizing by word and filtering stopwords\n",
    "all_tokens = [w for w in word_tokenize(text.lower()) if not w in stopwords]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Preprocess the text data\n",
    "tokenized_corpus = []\n",
    "for text in corpus:\n",
    "    # Tokenize and lowercase\n",
    "    tokens = [w for w in word_tokenize(text.lower()) if not w in stopwords]\n",
    "    tokenized_corpus.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04121411  0.05559197  0.00678513  0.10476194  0.06068745 -0.04715254\n",
      "  0.09819283  0.06152793 -0.09428605 -0.00808557 -0.00302357 -0.11015326\n",
      "  0.01054146 -0.03424115  0.01499417  0.04945535 -0.01193945 -0.09450546\n",
      " -0.02136887 -0.22999683 -0.00865448  0.09604455  0.090791   -0.04820855\n",
      " -0.01844855  0.08168797 -0.04979371 -0.03562324 -0.12227615  0.01029273\n",
      "  0.07659133  0.02646962  0.06090303 -0.02522595 -0.11305182  0.10907504\n",
      "  0.00245716  0.0135058  -0.05678719 -0.12770694  0.09332517 -0.03349303\n",
      " -0.0266207  -0.04431875  0.07680757 -0.03712076 -0.07177722  0.06321445\n",
      "  0.08723816  0.00505102 -0.00217429 -0.0271461   0.01802908  0.01231637\n",
      " -0.0591119   0.05627663  0.00839548 -0.09682135 -0.03382175 -0.03909757\n",
      "  0.00901577  0.05301319  0.09591538  0.02624076 -0.03380971  0.02053177\n",
      " -0.12669718  0.02296695 -0.07811047 -0.01725884 -0.00794656  0.09267309\n",
      "  0.0142447  -0.03025072  0.02834588 -0.02415352  0.0151986  -0.08246949\n",
      " -0.04352848 -0.04295498 -0.00553168 -0.07532179 -0.01674689  0.01533538\n",
      "  0.03217728 -0.05245645 -0.02903465 -0.01770962  0.03687425  0.00896427\n",
      "  0.02391264  0.11337955 -0.04997768  0.03511639  0.14887708  0.02625461\n",
      " -0.04299884 -0.08341297  0.04465487  0.01550675]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "# Preprocess the text data\n",
    "tokenized_corpus = []\n",
    "for text in corpus:\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    tokenized_corpus.append(tokens)\n",
    "\n",
    "context_windows = []\n",
    "window_size = 4  # Size of the context window\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    sentence_length = len(sentence)\n",
    "    for i in range(sentence_length):\n",
    "        # Extract the target word\n",
    "        target_word = sentence[i]\n",
    "\n",
    "        # Extract the context words within the window size\n",
    "        context_words = []\n",
    "        for j in range(max(0, i - window_size), min(sentence_length, i + window_size + 1)):\n",
    "            if j != i:\n",
    "                context_words.append(sentence[j])\n",
    "\n",
    "        # Append the target word and its context words as a tuple\n",
    "        # Convert context_words to tuple\n",
    "        context_windows.append((target_word, tuple(context_words)))\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(context_windows, window=5,\n",
    "                 min_count=10, sg=0)  # CBOW approach\n",
    "\n",
    "# Accessing trained word vectors\n",
    "word_vector = model.wv['breathe']  # Get the word vector for a specific word\n",
    "print(word_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
