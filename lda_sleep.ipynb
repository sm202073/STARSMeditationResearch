{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c73ba532-dd42-4aa7-b199-214a66b1b871",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../SleepMeditations/Sleep - 33.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sana.madhavan/Desktop/STARSMeditationResearch/lda_sleep.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sana.madhavan/Desktop/STARSMeditationResearch/lda_sleep.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m (file \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.ipynb_checkpoints\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sana.madhavan/Desktop/STARSMeditationResearch/lda_sleep.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sana.madhavan/Desktop/STARSMeditationResearch/lda_sleep.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m../SleepMeditations/\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m file, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sana.madhavan/Desktop/STARSMeditationResearch/lda_sleep.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m          encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39municode_escape\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m infile:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sana.madhavan/Desktop/STARSMeditationResearch/lda_sleep.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     content \u001b[39m=\u001b[39m infile\u001b[39m.\u001b[39mread()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sana.madhavan/Desktop/STARSMeditationResearch/lda_sleep.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     t\u001b[39m#ext = rtf_to_text(content)\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../SleepMeditations/Sleep - 33.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "\n",
    "file_names = os.listdir('./SleepMeditations/')\n",
    "# Create Dictionary for File Name and Text\n",
    "file_name_and_text = {}\n",
    "for file in file_names:\n",
    "    if (file == \".ipynb_checkpoints\"):\n",
    "        continue\n",
    "\n",
    "    with open('../SleepMeditations/' + file, \"r\",\n",
    "             encoding='unicode_escape') as infile:\n",
    "        content = infile.read()\n",
    "        #text = rtf_to_text(content)\n",
    "        file_name_and_text[file] = content\n",
    "file_data = (pd.DataFrame.from_dict(file_name_and_text, orient='index')\n",
    "             .reset_index().rename(index=str, columns={'index': 'file_name', 0: 'text'}))\n",
    "\n",
    "file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba8676-7816-4c89-812a-afa7ba539a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee0e5a09-4b5b-483a-8ae5-a91c11dd6437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anushree/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68b79cac-c2b7-4f87-bc96-834fd071ef08",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-722d981b2819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_stop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m def clean_tweets(df=file_data, \n\u001b[0m\u001b[1;32m      6\u001b[0m                  \u001b[0mtext_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 ):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_data' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "en_stop_words = set(stopwords.words('english'))\n",
    "list(en_stop_words)[:10]\n",
    "\n",
    "def clean_tweets(df=file_data, \n",
    "                 text_col='text', \n",
    "                ):\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # drop rows with empty values\n",
    "    df_copy.dropna(inplace=True)\n",
    "    \n",
    "    # format the date\n",
    "    #df_copy[date_col] = df_copy[date_col].apply(lambda row: datetime.strptime(row, '%m-%d-%Y %H:%M:%S'))\n",
    "    \n",
    "    # filter rows older than a given date\n",
    "    #df_copy = df_copy[df_copy[date_col] >=start_datetime]\n",
    "    \n",
    "    # lower the tweets\n",
    "    df_copy['preprocessed_' + text_col] = df_copy[text_col].str.lower()\n",
    "    \n",
    "    # filter out stop words and URLs\n",
    "    en_stop_words = set(stopwords.words('english'))\n",
    "    en_stop_words.add('music')\n",
    "    en_stop_words.add('Jason Stevenson')\n",
    "    extended_stop_words = en_stop_words | \\\n",
    "                        {\n",
    "                            '&amp;', 'rt',                           \n",
    "                            'th','co', 're', 've', 'kim', 'daca'\n",
    "                        }\n",
    "    url_re = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'        \n",
    "    df_copy['preprocessed_' + text_col] = df_copy['preprocessed_' + text_col].apply(lambda row: ' '.join(\n",
    "        [word for word in row.split() if (not word in en_stop_words) and (not re.match(url_re, word))]))\n",
    "    \n",
    "    # tokenize the tweets\n",
    "    tokenizer = RegexpTokenizer('[a-zA-Z]\\w+\\'?\\w*')\n",
    "    df_copy['tokenized_' + text_col] = df_copy['preprocessed_' + text_col].apply(lambda row: tokenizer.tokenize(row))\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "#df_tweets = pd.read_csv('trump_tweets.csv')\n",
    "df_clean = clean_tweets(file_data)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd1d4979-55bf-45ec-b007-1a46ff0f4261",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-68ba8b76ff9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mget_most_freq_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_text\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_clean' is not defined"
     ]
    }
   ],
   "source": [
    "def get_most_freq_words(str, n=None):\n",
    "    vect = CountVectorizer().fit(str)\n",
    "    bag_of_words = vect.transform(str)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]\n",
    "    freq =sorted(freq, key = lambda x: x[1], reverse=True)\n",
    "    return freq[:n]\n",
    "  \n",
    "get_most_freq_words([ word for line in df_clean.tokenized_text for word in line],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97fcf0aa-325b-4230-8375-9c352642197f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d0cd983a33f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# build a dictionary where for each tweet, each word has its own id.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# We have 6882 tweets and 10893 words in the dictionary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtweets_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# build the corpus i.e. vectors with the number of occurence of each word per tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_clean' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# build a dictionary where for each tweet, each word has its own id.\n",
    "# We have 6882 tweets and 10893 words in the dictionary.\n",
    "tweets_dictionary = Dictionary(df_clean.tokenized_text)\n",
    "\n",
    "# build the corpus i.e. vectors with the number of occurence of each word per tweet\n",
    "tweets_corpus = [tweets_dictionary.doc2bow(\n",
    "    tweet) for tweet in df_clean.tokenized_text]\n",
    "\n",
    "# compute coherence\n",
    "tweets_coherence = []\n",
    "for nb_topics in range(1, 36):\n",
    "    lda = LdaModel(tweets_corpus, num_topics=nb_topics,\n",
    "                   id2word=tweets_dictionary, passes=10)\n",
    "    cohm = CoherenceModel(model=lda, corpus=tweets_corpus,\n",
    "                          dictionary=tweets_dictionary, coherence='u_mass')\n",
    "    coh = cohm.get_coherence()\n",
    "    tweets_coherence.append(coh)\n",
    "\n",
    "# visualize coherence\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, 36), tweets_coherence)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9203fe49-0d7a-4eb1-80fe-382c2d9a251c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweets_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2ebe254ca743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m tweets_lda = LdaModel(tweets_corpus, num_topics=k,\n\u001b[0m\u001b[1;32m      5\u001b[0m                       id2word=tweets_dictionary, passes=10)\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweets_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "k = 6\n",
    "tweets_lda = LdaModel(tweets_corpus, num_topics=k,\n",
    "                      id2word=tweets_dictionary, passes=10)\n",
    "\n",
    "\n",
    "def plot_top_words(lda=tweets_lda, nb_topics=k, nb_words=10):\n",
    "    top_words = [[word for word, _ in lda.show_topic(\n",
    "        topic_id, topn=50)] for topic_id in range(lda.num_topics)]\n",
    "    top_betas = [[beta for _, beta in lda.show_topic(\n",
    "        topic_id, topn=50)] for topic_id in range(lda.num_topics)]\n",
    "\n",
    "    gs = gridspec.GridSpec(round(math.sqrt(k))+1, round(math.sqrt(k))+1)\n",
    "    gs.update(wspace=0.5, hspace=0.5)\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    for i in range(nb_topics):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.barh(range(nb_words), top_betas[i][:nb_words],\n",
    "                 align='center', color='blue', ecolor='black')\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_yticks(range(nb_words))\n",
    "        ax.set_yticklabels(top_words[i][:nb_words])\n",
    "        plt.title(\"Topic \"+str(i))\n",
    "\n",
    "\n",
    "plot_top_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4e40b-029b-4f63-a5aa-cd4731034904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d6094d-fce3-4ad1-a68f-251285369dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b9b27-3e60-4b91-90dc-856b50642180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
